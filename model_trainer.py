import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib
import os
import uuid

# -----------------------------
# Configuration
# -----------------------------
random_filename_model = "maruf_" + str(uuid.uuid4()) + ".h5"
random_filename_scaler = "maruf_" + str(uuid.uuid4()) + ".pkl"
# Make sure to update this to the actual file name generated by the Python script you ran previously
DATASET_FILE = "maruf_20251128115103_0rzgk9.csv" 
MODEL_OUTPUT = random_filename_model
SCALER_OUTPUT = random_filename_scaler

# Features must match CSV exactly
# CRITICAL: REMOVE "humidity" from the list
feature_cols = [
    "age",
    "gender",
    "weight",
    "humidity_scale", 
    "temperature",
    "complication",
    "is_indoors",
    "is_ground_wet",
    "is_windy_or_fanned",
    "is_direct_sun",
    "activity_type",
    "duration_minutes",
    "pace",
    "terrain_type",
    "sweat_level",
    "intensity_score"
]

# This will now be 16 features (down from 17)
INPUT_DIMENSION = len(feature_cols)

# -----------------------------
# Load dataset
# -----------------------------
if not os.path.exists(DATASET_FILE):
    # This assumes the dataset file created in the previous step is named similar to the example
    # Make sure you use the exact filename of the CSV you generated!
    raise FileNotFoundError(f"CSV file not found: {DATASET_FILE}. Please check the filename.")

df = pd.read_csv(DATASET_FILE)
print(f"✅ Loaded dataset: {DATASET_FILE}")

# Validate columns
missing = set(feature_cols) - set(df.columns)
if missing:
    raise ValueError(f"Missing columns in CSV: {missing}")

X = df[feature_cols].values
y = df["water_intake"].values

# -----------------------------
# Train/Test split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# -----------------------------
# Feature scaling
# -----------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# -----------------------------
# Model definition
# -----------------------------
model = tf.keras.Sequential([
    # Input dimension is now 16
    tf.keras.layers.Dense(256, activation="relu", input_shape=(INPUT_DIMENSION,)),
    tf.keras.layers.Dense(128, activation="relu"),
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dense(32, activation="relu"),
    tf.keras.layers.Dense(1, activation="relu") 
])

model.compile(optimizer="adam", loss="mse", metrics=["mae"])
model.summary()

# -----------------------------
# Training
# -----------------------------
history = model.fit(
    X_train_scaled, y_train,
    epochs=200,
    batch_size=32,
    validation_data=(X_test_scaled, y_test),
    verbose=1
)

# -----------------------------
# Evaluation
# -----------------------------
loss, mae = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"\n✅ Model Evaluation:")
print(f"  MSE: {loss:.2f}")
print(f"  MAE: {mae:.2f} ml")

# -----------------------------
# Save model & scaler
# -----------------------------
model.save(MODEL_OUTPUT)
joblib.dump(scaler, SCALER_OUTPUT)

print(f"\n✅ Model saved to: {MODEL_OUTPUT}")
print(f"✅ Scaler saved to: {SCALER_OUTPUT}")